quarkus.langchain4j.gpu-llama3.include-models-in-artifact=false

# Configure GPULlama3
quarkus.langchain4j.gpu-llama3.enable-integration=true
quarkus.langchain4j.gpu-llama3.chat-model.model-name=beehive-lab/Llama-3.2-1B-Instruct-GGUF
quarkus.langchain4j.gpu-llama3.chat-model.quantization=FP16
quarkus.langchain4j.gpu-llama3.chat-model.temperature=0.7
quarkus.langchain4j.gpu-llama3.chat-model.max-tokens=513

# other supported models:
#model-name=ggml-org/Qwen3-0.6B-GGUF
#quantization=f16

# Increase timeouts for vertx during development - useful for debugging
quarkus.vertx.warning-exception-time=30S
quarkus.vertx.max-event-loop-execute-time=30s
quarkus.vertx.blocked-thread-check-interval=10s

# Configure thread pools for model operations
quarkus.thread-pool.max-threads=20

# Enable verbose logging for debug purposes
%dev.quarkus.log.category."io.quarkiverse.langchain4j".level=DEBUG
%dev.quarkus.log.category."io.quarkiverse.langchain4j.gpullama3".level=DEBUG
%dev.quarkus.log.category."org.acme.example.gpullama3".level=DEBUG