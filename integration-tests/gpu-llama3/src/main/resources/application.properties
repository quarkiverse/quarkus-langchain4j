quarkus.langchain4j.gpu-llama3.include-models-in-artifact=false

# Configure GPULlama3
quarkus.langchain4j.gpu-llama3.enable-integration=true
quarkus.langchain4j.gpu-llama3.chat-model.model-name=beehive-lab/Llama-3.2-1B-Instruct-GGUF
quarkus.langchain4j.gpu-llama3.chat-model.quantization=FP16
quarkus.langchain4j.gpu-llama3.chat-model.temperature=0.7
quarkus.langchain4j.gpu-llama3.chat-model.max-tokens=513

# other supported models:
#model-name=ggml-org/Qwen3-0.6B-GGUF
#quantization=f16