= IBM watsonx.ai

include::./includes/attributes.adoc[]

You can develop generative AI solutions with foundation models in IBM watsonx.ai. You can use prompts to generate, classify, summarize, or extract content from your input text. Choose from IBM models or open source models from Hugging Face. You can tune foundation models to customize your prompt output or optimize inferencing performance.

IMPORTANT: Supported only for IBM watsonx as a service on link:https://www.ibm.com/products/watsonx-ai/foundation-models[IBM Cloud].

== Using watsonx.ai

To employ *watsonx.ai* LLMs, integrate the following dependency into your project:

[source,xml,subs=attributes+]
----
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-watsonx</artifactId>
    <version>{project-version}</version>
</dependency>
----

If no other extension is installed, xref:ai-services.adoc[AI Services] will automatically utilize the configured watsonx dependency.

=== Configuration
To use the watsonx.ai dependency, you must configure some required values in the `application.properties` file.

==== Base URL
The `base-url` property depends on the region of the provided service instance, use one of the following values:

* Dallas: https://us-south.ml.cloud.ibm.com
* Frankfurt: https://eu-de.ml.cloud.ibm.com
* Tokyo: https://jp-tok.ml.cloud.ibm.com
* London: https://eu-gb.ml.cloud.ibm.com

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.base-url=https://us-south.ml.cloud.ibm.com
----

==== Project ID
To prompt foundation models in watsonx.ai programmatically, you need to pass the identifier (ID) of a project.

To get the ID of a project, complete the following steps:

1. Open the project, and then click the Manage tab.
2. Copy the project ID from the Details section of the General page.

NOTE: To view the list of projects, go to https://dataplatform.cloud.ibm.com/projects/?context=wx.

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.project-id=23d...
----

==== API Key
To prompt foundation models in IBM watsonx.ai programmatically, you need an IBM Cloud API key.

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.api-key=hG-...
----

NOTE: To determine the API key, go to https://cloud.ibm.com/iam/apikeys and generate it.

==== Writing prompts

When creating prompts using *watsonx.ai*, it's important to follow the guidelines of the model you choose. Depending on the model, some special instructions may be required to ensure the desired output. For best results, always refer to the documentation provided for each model to maximize the effectiveness of your prompts.

By default, the *watsonx.ai* module uses the `prompt-formatter` property to simplify the prompt creation process. This property automatically handles the addition of tags to your prompts based on the requirements of the model, ensuring consistent and accurate prompt structures without additional effort. This functionality is particularly useful for models such as `ibm/granite-13b-chat-v2`, `meta-llama/llama-3-70b-instruct`, `mistralai/mistral-large` and other supported models.

This helps maintain prompt clarity and improves interaction with the LLM by ensuring that prompts follow the required structure. If, for any reason, you need to disable this functionality and manually manage tags, you can configure the `prompt-formatter` property in your `application.properties` file as follows:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.chat-model.prompt-formatter=false
----

For example, if you choose to disable the `prompt-formatter` and use `ibm/granite-13b-chat-v2`, you will need to manually add the `<|system|>`, `<|user|>`, and `<|assistant|>` instructions:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.api-key=hG-...
quarkus.langchain4j.watsonx.base-url=https://us-south.ml.cloud.ibm.com
quarkus.langchain4j.watsonx.chat-model.model-id=ibm/granite-13b-chat-v2
quarkus.langchain4j.watsonx.chat-model.prompt-formatter=false
----

[source,java]
----
@RegisterAiService
public interface LLMService {

    public record Result(Integer result) {}

    @SystemMessage("""
        <|system|>
        You are a calculator and you must perform the mathematical operation
        {response_schema}
        """)
    @UserMessage("""
        <|user|>
        {firstNumber} + {secondNumber}
        <|assistant|>
        """)
    public Result calculator(int firstNumber, int secondNumber);
}
----

When `prompt-formatter` is enabled, the system will format prompts automatically, without requiring tags:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.api-key=hG-...
quarkus.langchain4j.watsonx.base-url=https://us-south.ml.cloud.ibm.com
quarkus.langchain4j.watsonx.chat-model.model-id=ibm/granite-13b-chat-v2
----

[source,java]
----
@RegisterAiService
public interface LLMService {

    public record Result(Integer result) {}

    @SystemMessage("""
        You are a calculator and you must perform the mathematical operation
        {response_schema}
        """)
    @UserMessage("""
        {firstNumber} + {secondNumber}
        """)
    public Result calculator(int firstNumber, int secondNumber);
}
----

The `prompt-formatter` supports the following models:

* `mistralai/mistral-large`
* `mistralai/mixtral-8x7b-instruct-v01`
* `sdaia/allam-1-13b-instruct`
* `meta-llama/llama-3-405b-instruct`
* `meta-llama/llama-3-1-70b-instruct`
* `meta-llama/llama-3-1-8b-instruct`
* `meta-llama/llama-3-70b-instruct`
* `meta-llama/llama-3-8b-instruct`
* `ibm/granite-13b-chat-v2`
* `ibm/granite-13b-instruct-v2`
* `ibm/granite-7b-lab`
* `ibm/granite-20b-code-instruct`
* `ibm/granite-34b-code-instruct`
* `ibm/granite-3b-code-instruct`
* `ibm/granite-8b-code-instruct`

If a model that does not support the `prompt-formatter` is selected, a warning will be displayed during the application startup. In such cases, the prompt formatting will not be applied, and no tags will be automatically added to the prompts.

==== Tool Execution with Prompt Formatter

In addition to simplifying prompt creation, the `prompt-formatter` property also enables the execution of tools for specific models. Tools allow for dynamic interactions within the model, enabling the AI to perform specific actions or fetch data as part of its response.

When the `prompt-formatter` is enabled and a supported model is selected, the prompt will be automatically formatted to use the tools. More information about tools is available in the xref:./agent-and-tools.adoc[Agent and Tools] page.

Currently, the following models support tool execution:

* `mistralai/mistral-large`
* `meta-llama/llama-3-405b-instruct`
* `meta-llama/llama-3-1-70b-instruct`

IMPORTANT: The `@SystemMessage` and `@UserMessage` annotations are joined by default with a new line. If you want to change this behavior, use the property `quarkus.langchain4j.watsonx.chat-model.prompt-joiner=<value>`. By adjusting this property, you can define your preferred way of joining messages and ensure that the prompt structure meets your specific needs. This customization option is available only when the `prompt-formatter` property is set to `false`. When the `prompt-formatter` is enabled, the prompt formatting, including the addition of tags and message joining, is automatically handled. In this case, the `prompt-joiner` property will be ignored, and you will not have the ability to customize how messages are joined.

NOTE: Sometimes it may be useful to use the `quarkus.langchain4j.watsonx.chat-model.stop-sequences` property to prevent the LLM model from returning more results than desired.

==== All configuration properties

include::includes/quarkus-langchain4j-watsonx.adoc[leveloffset=+1,opts=optional]
