= GPULlama3.java Chat Models

include::./includes/attributes.adoc[]
include::./includes/customization.adoc[]

https://github.com/beehive-lab/GPULlama3.java[GPULlama3.java] provides a *Java-native implementation* of LLMs that runs entirely in Java and executes automatically on GPUs via https://github.com/beehive-lab/TornadoVM[TornadoVM].

This extension allows Quarkus applications to use locally hosted LLMs (*Llama3*, *Mistral*, *Qwen2.5*, *Deepseek-R1-Distill-Qwen*, *Qwen3*, *Phi3*, *IBM Granite 3.2+*, *IBM Granite 4.0*) for chat-based inference, leveraging GPU acceleration without requiring native CUDA code.

https://github.com/beehive-lab/gpullama3-quarkus-langchain4j-demo[Here] you can see a collection of demo Quarkus applications with GPULlama3.java!

[#_prerequisites]
== Prerequisites

=== Java Version and TornadoVM

GPULlama3.java requires TornadoVM and *Java 21 or later* due to its use of the https://openjdk.org/jeps/448[Java Vector API].

Install TornadoVM with SDKMAN!:

[source,shell]
----
sdk install tornadovm 2.2.0-opencl
----

Or, manually:

*Linux (x86_64)*

[source,shell]
----
wget https://github.com/beehive-lab/TornadoVM/releases/download/v2.2.0/tornadovm-2.2.0-opencl-linux-amd64.zip
unzip tornadovm-2.2.0-opencl-linux-amd64.zip

export TORNADOVM_HOME="$(pwd)/tornadovm-2.2.0-opencl"
export PATH=$TORNADOVM_HOME/bin:$PATH
----

*macOS (Apple Silicon)*

[source,shell]
----
wget https://github.com/beehive-lab/TornadoVM/releases/download/v2.2.0/tornadovm-2.2.0-opencl-mac-aarch64.zip
unzip tornadovm-2.2.0-opencl-mac-aarch64.zip

export TORNADOVM_HOME="$(pwd)/tornadovm-2.2.0-opencl"
export PATH=$TORNADOVM_HOME/bin:$PATH
----

To verify installation:
[source, shell]
----
tornado --devices
tornado --version
----

The TornadoVM installation:

- Sets the `TORNADOVM_HOME` environment variable to the TornadoVM SDK path.
- `TORNADOVM_HOME` contains the `tornado-argfile` with all the JVM arguments required to enable TornadoVM.
- ⚠️ The `tornado-argfile` should be used for *building* and *running* the Quarkus application (see section Building & Running the Quarkus Application).

== Using GPULlama3.java

To integrate the GPULlama3 chat model into your Quarkus application, add the following dependency:

[source,xml,subs=attributes+]
----
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-gpu-llama3</artifactId>
    <version>{project-version}</version>
</dependency>
----

IMPORTANT: If no other LLM extension is configured, xref:ai-services.adoc[AI Services] will automatically use the GPU-accelerated GPULlama3!

Sample implementation for ChatModel:
[source,java]
----
@Path("chat")
public class ChatLanguageModelResource {
    private final ChatModel chatModel;
    public ChatLanguageModelResource(ChatModel chatModel) {
        this.chatModel = chatModel;
    }

    @GET
    @Path("blocking")
    public String blocking() {
        return chatModel.chat("When was the nobel prize for economics first awarded?");
    }
}
----

Send requests to blocking endpoint:
```
curl http://localhost:8080/chat/blocking
```

Sample implementation for StreamingChatModel:
[source,java]
----
@Path("chat")
public class ChatLanguageModelResource {
    private final StreamingChatModel streamingChatModel;
    public ChatLanguageModelResource(StreamingChatModel streamingChatModel) {
        this.streamingChatModel = streamingChatModel;
    }

    @GET
    @Path("streaming")
    @RestStreamElementType(MediaType.TEXT_PLAIN)
    public Multi<String> streaming() {
        return Multi.createFrom().emitter(emitter -> {
            streamingChatModel.chat("When was the nobel prize for economics first awarded?",
                    new StreamingChatResponseHandler() {
                        @Override
                        public void onPartialResponse(String token) {
                            emitter.emit(token);
                        }

                        @Override
                        public void onError(Throwable error) {
                            emitter.fail(error);
                        }

                        @Override
                        public void onCompleteResponse(ChatResponse completeResponse) {
                            emitter.complete();
                        }
                    });
        });
    }
}
----

Send requests to streaming endpoint:
```
curl http://localhost:8080/chat/streaming
```

== Configure GPULlama3

The GPULlama3 extension can be configured via standard Quarkus properties:

[source,properties]
----
# Enable GPULlama3 integration
quarkus.langchain4j.gpu-llama3.enable-integration=true

# Select the default model
quarkus.langchain4j.gpu-llama3.chat-model.model-name=unsloth/Llama-3.2-1B-Instruct-GGUF
quarkus.langchain4j.gpu-llama3.chat-model.quantization=F16
quarkus.langchain4j.gpu-llama3.chat-model.temperature=0.7
quarkus.langchain4j.gpu-llama3.chat-model.max-tokens=1024
----

Model files are automatically downloaded from https://huggingface.co/beehive-lab[Beehive Lab HuggingFace] if not available locally.

== Building & Running the Quarkus Application

=== Dev Mode

To run your Quarkus application in **dev mode** with TornadoVM:

1. Ensure your `pom.xml` contains the `quarkus-langchain4j-gpu-llama3` dependency (shown earlier).

2. Add the TornadoVM argfile as a Maven property:
+
[source,xml]
----
<properties>
    <tornado.argfile>/path/to/tornado-argfile</tornado.argfile>
</properties>
----
+
3. Pass the argfile to the JVM in the plugin configuration for dev mode:
+
[source,xml]
----
<plugin>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-maven-plugin</artifactId>
    <configuration>
        <jvmArgs>@${tornado.argfile}</jvmArgs>
    </configuration>
</plugin>
----
+
4. Launch dev mode explicitly:
+
[source,shell]
----
mvn quarkus:dev
----

---

=== Production Mode

To build and run your application in **production mode**:

1. Build the Quarkus application:
+
[source,shell]
----
mvn clean package
----
+
2. Run the generated jar with the TornadoVM argfile:
+
[source,shell]
----
java @$TORNADOVM_HOME/tornado-argfile -jar target/quarkus-app/quarkus-run.jar
----

IMPORTANT: Ensure `TORNADOVM_SDK` and the `tornado-argfile` path are correctly set.


== Supported Models and Quantizations

The following models have been tested with GPULlama3.java and can be found in link:++https://huggingface.co/beehive-lab/collections[Beehive Lab's HuggingFace Collections].

[IMPORTANT]
====
Quantization format names are model-dependent. Some models require `F16`, others `fp16` (lowercase), as defined in their GGUF files.
Use **exactly** the spelling listed below, or the model may fail to load.
====

[cols="2a,1,3a", options="header"]
|===
| Model | Quantizations | Model Identifier (Hugging Face)

| **Llama 3.2 1B** | `F16`, `Q8_0` | `unsloth/Llama-3.2-1B-Instruct-GGUF`
| **Llama 3.2 3B** | `F16`, `Q8_0` | `unsloth/Llama-3.2-3B-Instruct-GGUF`
| **Llama 3.1 8B** | `fp16`, `Q8_0` | `brittlewis12/Meta-Llama-3.1-8B-Instruct-GGUF`
| **Mistral 7B** | `fp16`, `Q8_0` | `MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF`
| **Qwen2.5 0.5B** | `F16`, `Q8_0` | `bartowski/Qwen2.5-0.5B-Instruct-GGUF`
| **Qwen2.5 1.5B** | `fp16`, `Q8_0` | `Qwen/Qwen2.5-1.5B-Instruct-GGUF`
| **DeepSeek-R1 Distill Qwen 1.5B** | `F16`, `Q8_0` | `hdnh2006/DeepSeek-R1-Distill-Qwen-1.5B-GGUF`
| **DeepSeek-R1 Distill Qwen 7B** | `F16`, `Q8_0` | `XelotX/DeepSeek-R1-Distill-Qwen-7B-GGUF`
| **Qwen3 0.6B** | `F16`, `Q8_0` | `ggml-org/Qwen3-0.6B-GGUF`
| **Qwen3 1.7B** | `F16`, `Q8_0` | `ggml-org/Qwen3-1.7B-GGUF`
| **Qwen3 4B** | `F16`, `Q8_0` | `ggml-org/Qwen3-4B-GGUF`
| **Qwen3 8B** | `F16`, `Q8_0` | `ggml-org/Qwen3-8B-GGUF`
| **Phi 3 Mini 4k** | `fp16`| `microsoft/Phi-3-mini-4k-instruct-gguf`
| **Phi 3 Mini 4k**| `Q8_0` |`bartowski/Phi-3-mini-4k-instruct-GGUF`
| **Phi 3 Mini 128k** | `Q8_0` | `QuantFactory/Phi-3-mini-4k-instruct-GGUF`
| **Phi 3.1 Mini 128k** | `Q8_0` | `bartowski/Phi-3.1-mini-128k-instruct-GGUF`
| **Granite 3.2 2B** | `F16`, `Q8_0` | `ibm-research/granite-3.2-2b-instruct-GGUF`
| **Granite 3.2 8B** | `F16`, `Q8_0` | `ibm-research/granite-3.2-8b-instruct-GGUF`
| **Granite 3.3 2B** | `F16`, `Q8_0` | `ibm-research/granite-3.3-2b-instruct-GGUF`
| **Granite 3.3 8B** | `F16`, `Q8_0` | `ibm-research/granite-3.3-8b-instruct-GGUF`
| **Granite 4.0 1B** | `F16`, `Q8_0` | `ibm-research/granite-4.0-1b-instruct-GGUF`
|===

Each entry corresponds to a GGUF model tested to run on TornadoVM via GPULlama3.java.

== Configuration Reference

include::includes/quarkus-langchain4j-gpu-llama3.adoc[leveloffset=+1,opts=optional]


== Limitations

* TornadoVM currently does **not** support GraalVM Native Image builds.
* Ensure that the TornadoVM environment (`TORNADOVM_HOME` and `tornado-argfile`) is properly set before running Quarkus.
* Only Java 21 or newer versions are supported.